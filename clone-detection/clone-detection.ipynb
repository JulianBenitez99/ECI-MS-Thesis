{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4",
   "authorship_tag": "ABX9TyPNeE1DajbFL6ukMGFaHjmW"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-qIFr_cZLA5a",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1713309321640,
     "user_tz": 300,
     "elapsed": 5522,
     "user": {
      "displayName": "Julián Benítez Gutiérrez",
      "userId": "01894930060764521238"
     }
    },
    "outputId": "d9589fd2-e095-4ad5-9ee1-1225855db69b",
    "ExecuteTime": {
     "end_time": "2024-04-22T03:11:27.369623Z",
     "start_time": "2024-04-22T03:11:21.952210Z"
    }
   },
   "source": "!pip install torch transformers boto3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.10/site-packages (2.2.2)\r\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.39.3)\r\n",
      "Collecting boto3\r\n",
      "  Downloading boto3-1.34.88-py3-none-any.whl (139 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m139.3/139.3 KB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in ./.venv/lib/python3.10/site-packages (from torch) (4.11.0)\r\n",
      "Requirement already satisfied: sympy in ./.venv/lib/python3.10/site-packages (from torch) (1.12)\r\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from torch) (2024.3.1)\r\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from torch) (3.13.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.22.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2024.4.16)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in ./.venv/lib/python3.10/site-packages (from transformers) (0.4.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (24.0)\r\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.66.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.1)\r\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./.venv/lib/python3.10/site-packages (from transformers) (0.15.2)\r\n",
      "Collecting botocore<1.35.0,>=1.34.88\r\n",
      "  Downloading botocore-1.34.88-py3-none-any.whl (12.2 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m12.2/12.2 MB\u001B[0m \u001B[31m13.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting s3transfer<0.11.0,>=0.10.0\r\n",
      "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m82.2/82.2 KB\u001B[0m \u001B[31m9.2 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hCollecting jmespath<2.0.0,>=0.7.1\r\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./.venv/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.88->boto3) (2.9.0.post0)\r\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in ./.venv/lib/python3.10/site-packages (from botocore<1.35.0,>=1.34.88->boto3) (2.2.1)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\r\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.venv/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\r\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.88->boto3) (1.16.0)\r\n",
      "Installing collected packages: jmespath, botocore, s3transfer, boto3\r\n",
      "Successfully installed boto3-1.34.88 botocore-1.34.88 jmespath-1.0.1 s3transfer-0.10.1\r\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-22T03:21:22.959422Z",
     "start_time": "2024-04-22T03:21:22.180367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "TEST_REPOSITORY = \"mislav/hub\"\n",
    "BUCKET = \"go-project-functions\"\n",
    "METADATA = \"metadata.json\"\n",
    "FUNCTIONS = \"go-functions.json\"\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# response = s3.get_object(Bucket=BUCKET, Key=f\"{TEST_REPOSITORY}/{METADATA}\")\n",
    "# metadata = response['Body'].read().decode('utf-8')\n",
    "# print(metadata)\n",
    "\n",
    "response = s3.get_object(Bucket=BUCKET, Key=f\"{TEST_REPOSITORY}/{FUNCTIONS}\")\n",
    "functions = response['Body'].read().decode('utf-8')\n",
    "functions = json.loads(functions)\n",
    "\n",
    "print(functions[0])\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'code': 'func (cmd Cmd) String() string {\\n\\targs := make([]string, len(cmd.Args))\\n\\tfor i, a := range cmd.Args {\\n\\t\\tif strings.ContainsRune(a, \\'\"\\') {\\n\\t\\t\\targs[i] = fmt.Sprintf(`\\'%s\\'`, a)\\n\\t\\t} else if a == \"\" || strings.ContainsRune(a, \\'\\\\\\'\\') || strings.ContainsRune(a, \\' \\') {\\n\\t\\t\\targs[i] = fmt.Sprintf(`\"%s\"`, a)\\n\\t\\t} else {\\n\\t\\t\\targs[i] = a\\n\\t\\t}\\n\\t}\\n\\treturn fmt.Sprintf(\"%s %s\", cmd.Name, strings.Join(args, \" \"))\\n}', 'id': 'ab6a64cc-364b-4189-af19-eaf29069c387', 'line': 23, 'path': '/tmp/temp-repo/cmd/cmd.go'}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT license.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig\n",
    "\n",
    "class UniXcoder(nn.Module):\n",
    "    def __init__(self, model_name):\n",
    "        \"\"\"\n",
    "            Build UniXcoder.\n",
    "\n",
    "            Parameters:\n",
    "\n",
    "            * `model_name`- huggingface model card name. e.g. microsoft/unixcoder-base\n",
    "        \"\"\"\n",
    "        super(UniXcoder, self).__init__()\n",
    "        self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        self.config = RobertaConfig.from_pretrained(model_name)\n",
    "        self.config.is_decoder = True\n",
    "        self.model = RobertaModel.from_pretrained(model_name, config=self.config)\n",
    "\n",
    "        self.register_buffer(\"bias\", torch.tril(torch.ones((1024, 1024), dtype=torch.uint8)).view(1,1024, 1024))\n",
    "        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)\n",
    "        self.lm_head.weight = self.model.embeddings.word_embeddings.weight\n",
    "        self.lsm = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "        self.tokenizer.add_tokens([\"<mask0>\"],special_tokens=True)\n",
    "\n",
    "    def tokenize(self, inputs, mode=\"<encoder-only>\", max_length=512, padding=False):\n",
    "        \"\"\"\n",
    "        Convert string to token ids\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        * `inputs`- list of input strings.\n",
    "        * `max_length`- The maximum total source sequence length after tokenization.\n",
    "        * `padding`- whether to pad source sequence length to max_length.\n",
    "        * `mode`- which mode the sequence will use. i.e. <encoder-only>, <decoder-only>, <encoder-decoder>\n",
    "        \"\"\"\n",
    "        assert mode in [\"<encoder-only>\", \"<decoder-only>\", \"<encoder-decoder>\"]\n",
    "        assert max_length < 1024\n",
    "\n",
    "        tokenizer = self.tokenizer\n",
    "\n",
    "        tokens_ids = []\n",
    "        for x in inputs:\n",
    "            tokens = tokenizer.tokenize(x)\n",
    "            if mode == \"<encoder-only>\":\n",
    "                tokens = tokens[:max_length-4]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n",
    "            elif mode == \"<decoder-only>\":\n",
    "                tokens = tokens[-(max_length-3):]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens\n",
    "            else:\n",
    "                tokens = tokens[:max_length-5]\n",
    "                tokens = [tokenizer.cls_token,mode,tokenizer.sep_token] + tokens + [tokenizer.sep_token]\n",
    "\n",
    "            tokens_id = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            if padding:\n",
    "                tokens_id = tokens_id + [self.config.pad_token_id] * (max_length-len(tokens_id))\n",
    "            tokens_ids.append(tokens_id)\n",
    "        return tokens_ids\n",
    "\n",
    "    def decode(self, source_ids):\n",
    "        \"\"\" Convert token ids to string \"\"\"\n",
    "        predictions = []\n",
    "        for x in source_ids:\n",
    "            prediction = []\n",
    "            for y in x:\n",
    "                t = y.cpu().numpy()\n",
    "                t = list(t)\n",
    "                if 0 in t:\n",
    "                    t = t[:t.index(0)]\n",
    "                text = self.tokenizer.decode(t,clean_up_tokenization_spaces=False)\n",
    "                prediction.append(text)\n",
    "            predictions.append(prediction)\n",
    "        return predictions\n",
    "\n",
    "    def forward(self, source_ids):\n",
    "        \"\"\" Obtain token embeddings and sentence embeddings \"\"\"\n",
    "        mask = source_ids.ne(self.config.pad_token_id)\n",
    "        token_embeddings = self.model(source_ids,attention_mask = mask.unsqueeze(1) * mask.unsqueeze(2))[0]\n",
    "        sentence_embeddings = (token_embeddings * mask.unsqueeze(-1)).sum(1) / mask.sum(-1).unsqueeze(-1)\n",
    "        return token_embeddings, sentence_embeddings\n",
    "\n",
    "    def generate(self, source_ids, decoder_only = True, eos_id = None, beam_size = 5, max_length = 64):\n",
    "        \"\"\" Generate sequence given context (source_ids) \"\"\"\n",
    "\n",
    "        # Set encoder mask attention matrix: bidirectional for <encoder-decoder>, unirectional for <decoder-only>\n",
    "        if decoder_only:\n",
    "            mask = self.bias[:,:source_ids.size(-1),:source_ids.size(-1)]\n",
    "        else:\n",
    "            mask = source_ids.ne(self.config.pad_token_id)\n",
    "            mask = mask.unsqueeze(1) * mask.unsqueeze(2)\n",
    "\n",
    "        if eos_id is None:\n",
    "            eos_id = self.config.eos_token_id\n",
    "\n",
    "        device = source_ids.device\n",
    "\n",
    "        # Decoding using beam search\n",
    "        preds = []\n",
    "        zero = torch.LongTensor(1).fill_(0).to(device)\n",
    "        source_len = list(source_ids.ne(1).sum(-1).cpu().numpy())\n",
    "        length = source_ids.size(-1)\n",
    "        encoder_output = self.model(source_ids,attention_mask=mask)\n",
    "        for i in range(source_ids.shape[0]):\n",
    "            context = [[x[i:i+1,:,:source_len[i]].repeat(beam_size,1,1,1) for x in y]\n",
    "                     for y in encoder_output.past_key_values]\n",
    "            beam = Beam(beam_size,eos_id,device)\n",
    "            input_ids = beam.getCurrentState().clone()\n",
    "            context_ids = source_ids[i:i+1,:source_len[i]].repeat(beam_size,1)\n",
    "            out = encoder_output.last_hidden_state[i:i+1,:source_len[i]].repeat(beam_size,1,1)\n",
    "            for _ in range(max_length):\n",
    "                if beam.done():\n",
    "                    break\n",
    "                if _ == 0:\n",
    "                    hidden_states = out[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids = beam.getCurrentState().clone()\n",
    "                else:\n",
    "                    length = context_ids.size(-1)+input_ids.size(-1)\n",
    "                    out = self.model(input_ids,attention_mask=self.bias[:,context_ids.size(-1):length,:length],\n",
    "                                       past_key_values=context).last_hidden_state\n",
    "                    hidden_states = out[:,-1,:]\n",
    "                    out = self.lsm(self.lm_head(hidden_states)).data\n",
    "                    beam.advance(out)\n",
    "                    input_ids.data.copy_(input_ids.data.index_select(0, beam.getCurrentOrigin()))\n",
    "                    input_ids = torch.cat((input_ids,beam.getCurrentState().clone()),-1)\n",
    "            hyp = beam.getHyp(beam.getFinal())\n",
    "            pred = beam.buildTargetTokens(hyp)[:beam_size]\n",
    "            pred = [torch.cat([x.view(-1) for x in p]+[zero]*(max_length-len(p))).view(1,-1) for p in pred]\n",
    "            preds.append(torch.cat(pred,0).unsqueeze(0))\n",
    "\n",
    "        preds = torch.cat(preds,0)\n",
    "\n",
    "        return preds\n",
    "\n",
    "\n",
    "\n",
    "class Beam(object):\n",
    "    def __init__(self, size, eos, device):\n",
    "        self.size = size\n",
    "        self.device = device\n",
    "        # The score for each translation on the beam.\n",
    "        self.scores = torch.FloatTensor(size).zero_().to(device)\n",
    "        # The backpointers at each time-step.\n",
    "        self.prevKs = []\n",
    "        # The outputs at each time-step.\n",
    "        self.nextYs = [torch.LongTensor(size).fill_(0).to(device)]\n",
    "        # Has EOS topped the beam yet.\n",
    "        self._eos = eos\n",
    "        self.eosTop = False\n",
    "        # Time and k pair for finished.\n",
    "        self.finished = []\n",
    "\n",
    "    def getCurrentState(self):\n",
    "        \"Get the outputs for the current timestep.\"\n",
    "        batch = self.nextYs[-1].view(-1, 1)\n",
    "        return batch\n",
    "\n",
    "    def getCurrentOrigin(self):\n",
    "        \"Get the backpointers for the current timestep.\"\n",
    "        return self.prevKs[-1]\n",
    "\n",
    "    def advance(self, wordLk):\n",
    "        \"\"\"\n",
    "        Given prob over words for every last beam `wordLk` and attention\n",
    "        `attnOut`: Compute and update the beam search.\n",
    "\n",
    "        Parameters:\n",
    "\n",
    "        * `wordLk`- probs of advancing from the last step (K x words)\n",
    "        * `attnOut`- attention at the last step\n",
    "\n",
    "        Returns: True if beam search is complete.\n",
    "        \"\"\"\n",
    "        numWords = wordLk.size(1)\n",
    "\n",
    "        # Sum the previous scores.\n",
    "        if len(self.prevKs) > 0:\n",
    "            beamLk = wordLk + self.scores.unsqueeze(1).expand_as(wordLk)\n",
    "\n",
    "            # Don't let EOS have children.\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] == self._eos:\n",
    "                    beamLk[i] = -1e20\n",
    "        else:\n",
    "            beamLk = wordLk[0]\n",
    "        flatBeamLk = beamLk.view(-1)\n",
    "        bestScores, bestScoresId = flatBeamLk.topk(self.size, 0, True, True)\n",
    "\n",
    "        self.scores = bestScores\n",
    "\n",
    "        # bestScoresId is flattened beam x word array, so calculate which\n",
    "        # word and beam each score came from\n",
    "        prevK = torch.div(bestScoresId, numWords, rounding_mode=\"floor\")\n",
    "        self.prevKs.append(prevK)\n",
    "        self.nextYs.append((bestScoresId - prevK * numWords))\n",
    "\n",
    "\n",
    "        for i in range(self.nextYs[-1].size(0)):\n",
    "            if self.nextYs[-1][i] == self._eos:\n",
    "                s = self.scores[i]\n",
    "                self.finished.append((s, len(self.nextYs) - 1, i))\n",
    "\n",
    "        # End condition is when top-of-beam is EOS and no global score.\n",
    "        if self.nextYs[-1][0] == self._eos:\n",
    "            self.eosTop = True\n",
    "\n",
    "    def done(self):\n",
    "        return self.eosTop and len(self.finished) >= self.size\n",
    "\n",
    "    def getFinal(self):\n",
    "        if len(self.finished) == 0:\n",
    "            self.finished.append((self.scores[0], len(self.nextYs) - 1, 0))\n",
    "        self.finished.sort(key=lambda a: -a[0])\n",
    "        if len(self.finished) != self.size:\n",
    "            unfinished=[]\n",
    "            for i in range(self.nextYs[-1].size(0)):\n",
    "                if self.nextYs[-1][i] != self._eos:\n",
    "                    s = self.scores[i]\n",
    "                    unfinished.append((s, len(self.nextYs) - 1, i))\n",
    "            unfinished.sort(key=lambda a: -a[0])\n",
    "            self.finished+=unfinished[:self.size-len(self.finished)]\n",
    "        return self.finished[:self.size]\n",
    "\n",
    "    def getHyp(self, beam_res):\n",
    "        \"\"\"\n",
    "        Walk back to construct the full hypothesis.\n",
    "        \"\"\"\n",
    "        hyps=[]\n",
    "        for _,timestep, k in beam_res:\n",
    "            hyp = []\n",
    "            for j in range(len(self.prevKs[:timestep]) - 1, -1, -1):\n",
    "                hyp.append(self.nextYs[j+1][k])\n",
    "                k = self.prevKs[j][k]\n",
    "            hyps.append(hyp[::-1])\n",
    "        return hyps\n",
    "\n",
    "    def buildTargetTokens(self, preds):\n",
    "        sentence=[]\n",
    "        for pred in preds:\n",
    "            tokens = []\n",
    "            for tok in pred:\n",
    "                if tok==self._eos:\n",
    "                    break\n",
    "                tokens.append(tok)\n",
    "            sentence.append(tokens)\n",
    "        return sentence\n"
   ],
   "metadata": {
    "id": "S21WHFggLNJ6",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1713309341362,
     "user_tz": 300,
     "elapsed": 10567,
     "user": {
      "displayName": "Julián Benítez Gutiérrez",
      "userId": "01894930060764521238"
     }
    },
    "ExecuteTime": {
     "end_time": "2024-04-22T03:11:27.395876Z",
     "start_time": "2024-04-22T03:11:27.372122Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = UniXcoder(\"Lazyhope/unixcoder-clone-detection\")\n",
    "model.to(device)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnuVpNZtMeEo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1713309742029,
     "user_tz": 300,
     "elapsed": 1721,
     "user": {
      "displayName": "Julián Benítez Gutiérrez",
      "userId": "01894930060764521238"
     }
    },
    "outputId": "1114f8ed-25b0-4497-f539-bfb6a46108a8",
    "ExecuteTime": {
     "end_time": "2024-04-22T03:11:29.384393Z",
     "start_time": "2024-04-22T03:11:27.397278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UniXcoder(\n",
       "  (model): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(51416, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(1026, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(10, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): RobertaPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51416, bias=False)\n",
       "  (lsm): LogSoftmax(dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# func = \"func sum(a, b int) int {return a+b}\"\n",
    "func = functions[0][\"code\"]\n",
    "print(func)\n",
    "# encode first function\n",
    "tokens_ids = model.tokenize([func],max_length=512,mode=\"<encoder-only>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "tokens_embeddings,f_func_embedding = model(source_ids)\n",
    "\n",
    "# which is actually another code\n",
    "# nl = \"func sayHello() {return \\\"hello!\\\"}\"\n",
    "nl = functions[1][\"code\"]\n",
    "print(nl)\n",
    "tokens_ids = model.tokenize([nl],max_length=512,mode=\"<encoder-only>\")\n",
    "source_ids = torch.tensor(tokens_ids).to(device)\n",
    "tokens_embeddings,nl_embedding = model(source_ids)\n",
    "\n",
    "norm_f_func_embedding = torch.nn.functional.normalize(f_func_embedding, p=2, dim=1)\n",
    "norm_nl_embedding = torch.nn.functional.normalize(nl_embedding, p=2, dim=1)\n",
    "\n",
    "f_func_nl_similarity = torch.einsum(\"ac,bc->ab\",norm_f_func_embedding,norm_nl_embedding)\n",
    "\n",
    "# print(f_func_embedding.shape)\n",
    "# print(f_func_embedding)\n",
    "\n",
    "print(f_func_nl_similarity)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bRCzrYlIM_2v",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1713309840024,
     "user_tz": 300,
     "elapsed": 299,
     "user": {
      "displayName": "Julián Benítez Gutiérrez",
      "userId": "01894930060764521238"
     }
    },
    "outputId": "bf7606e4-63f6-4444-a0a6-b656cb5e1192",
    "ExecuteTime": {
     "end_time": "2024-04-22T03:22:24.879430Z",
     "start_time": "2024-04-22T03:22:22.241449Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func (cmd Cmd) String() string {\n",
      "\targs := make([]string, len(cmd.Args))\n",
      "\tfor i, a := range cmd.Args {\n",
      "\t\tif strings.ContainsRune(a, '\"') {\n",
      "\t\t\targs[i] = fmt.Sprintf(`'%s'`, a)\n",
      "\t\t} else if a == \"\" || strings.ContainsRune(a, '\\'') || strings.ContainsRune(a, ' ') {\n",
      "\t\t\targs[i] = fmt.Sprintf(`\"%s\"`, a)\n",
      "\t\t} else {\n",
      "\t\t\targs[i] = a\n",
      "\t\t}\n",
      "\t}\n",
      "\treturn fmt.Sprintf(\"%s %s\", cmd.Name, strings.Join(args, \" \"))\n",
      "}\n",
      "func (cmd *Cmd) WithArg(arg string) *Cmd {\n",
      "\tcmd.Args = append(cmd.Args, arg)\n",
      "\treturn cmd\n",
      "}\n",
      "tensor([[0.4797]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "execution_count": 17
  }
 ]
}
